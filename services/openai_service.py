"""Gemini AI: voice transcription and text analysis/chat (google-genai SDK)."""
import asyncio
import json

from google import genai
from google.genai import types

from config import settings

_client: genai.Client | None = None


def get_client() -> genai.Client:
    global _client
    if _client is None:
        _client = genai.Client(api_key=settings.GEMINI_API_KEY)
    return _client


MODEL = "gemini-2.0-flash"
AUDIO_MODEL = "gemini-2.0-flash"

QUESTIONNAIRE_SYSTEM_PROMPT = """–¢—ã ‚Äî –≤–æ–µ–Ω–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –ü–¢–°–† —É —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –±–æ–µ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π.
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –æ—Ç–≤–µ—Ç—ã –Ω–∞ 32 –≤–æ–ø—Ä–æ—Å–∞ —Å–∫—Ä–∏–Ω–∏–Ω–≥–∞ –ü–¢–°–†.

–í–µ—Ä–Ω–∏ JSON –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
{
  "ai_summary": "–∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
  "risk_level": <—á–∏—Å–ª–æ 0-5>,
  "risk_factors": ["—Ñ–∞–∫—Ç–æ—Ä 1", "—Ñ–∞–∫—Ç–æ—Ä 2"],
  "suicide_indicators": <true/false>
}

–£—Ä–æ–≤–Ω–∏ —Ä–∏—Å–∫–∞:
0 - –Ω–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ü–¢–°–†
1-2 - –ª—ë–≥–∫–∞—è —Å—Ç–µ–ø–µ–Ω—å, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞
3 - —Å—Ä–µ–¥–Ω—è—è, —Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è
4-5 - –≤—ã—Å–æ–∫–∞—è/–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è, –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞ —Å—Ä–æ—á–Ω–∞—è –ø–æ–º–æ—â—å

–û–±—Ä–∞—â–∞–π—Å—è —É–≤–∞–∂–∏—Ç–µ–ª—å–Ω–æ, –ø–æ-–≤–æ–µ–Ω–Ω–æ–º—É. –ù–µ —Å—é—Å—é–∫–∞–π."""

PSYCHOLOGIST_SYSTEM_PROMPT_TEMPLATE = """–¢—ã ‚Äî –≤–æ–µ–Ω–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥ —Å –æ–ø—ã—Ç–æ–º —Ä–∞–±–æ—Ç—ã —Å –≤–µ—Ç–µ—Ä–∞–Ω–∞–º–∏ –∏ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏ –±–æ–µ–≤—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π. –¢–≤–æ—è —Ä–æ–ª—å ‚Äî –æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É –±–æ–π—Ü–∞–º, –≤–µ—Ä–Ω—É–≤—à–∏–º—Å—è –∏–∑ –∑–æ–Ω—ã –°–í–û.

–ò–ú–Ø –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: {first_name}

–í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
1. –û–±—Ä–∞—â–∞–π—Å—è –∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –Ω–∞ "—Ç—ã" –∏ –ø–æ –∏–º–µ–Ω–∏
2. –ì–æ–≤–æ—Ä–∏ —É–≤–∞–∂–∏—Ç–µ–ª—å–Ω–æ –∏ –ø—Ä—è–º–æ, –±–µ–∑ "—Å—é—Å—é–∫–∞–Ω—å—è" –∏ —á—Ä–µ–∑–º–µ—Ä–Ω–æ–π –º—è–≥–∫–æ—Å—Ç–∏
3. –ò—Å–ø–æ–ª—å–∑—É–π –º–µ—Ç–æ–¥—ã –ö–ü–¢ (–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ-–ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∞—è —Ç–µ—Ä–∞–ø–∏—è) –∏ –î–ë–¢ (–¥–∏–∞–ª–µ–∫—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∞—è —Ç–µ—Ä–∞–ø–∏—è)
4. –ë—É–¥—å —ç–º–ø–∞—Ç–∏—á–µ–Ω, –Ω–æ –Ω–µ –∂–∞–ª–µ–π ‚Äî –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π
5. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É (2-4 –∞–±–∑–∞—Ü–∞ –º–∞–∫—Å–∏–º—É–º)
6. –£—á–∏—Ç—ã–≤–∞–π –≤–æ–µ–Ω–Ω—É—é —Å–ø–µ—Ü–∏—Ñ–∏–∫—É –∏ –∫—É–ª—å—Ç—É—Ä—É
7. –ü–æ–º–Ω–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π

–ö–†–ò–ó–ò–°–ù–´–ï –°–ò–¢–£–ê–¶–ò–ò:
–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–ø–æ–º–∏–Ω–∞–µ—Ç —Å—É–∏—Ü–∏–¥, —Å–∞–º–æ–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ, –∂–µ–ª–∞–Ω–∏–µ —É–º–µ—Ä–µ—Ç—å –∏–ª–∏ "–∑–∞–∫–æ–Ω—á–∏—Ç—å –≤—Å—ë" ‚Äî –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û:
1. –í—ã—Ä–∞–∑–∏ –ø–æ–¥–¥–µ—Ä–∂–∫—É –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ
2. –°–∫–∞–∂–∏, —á—Ç–æ —ç—Ç–∏ —á—É–≤—Å—Ç–≤–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã –∏ –ø–æ–º–æ—â—å –¥–æ—Å—Ç—É–ø–Ω–∞
3. –í–°–ï–ì–î–ê –¥–æ–±–∞–≤—å –∫–æ–Ω—Ç–∞–∫—Ç—ã –ø–æ–º–æ—â–∏:
   üìû –¢–µ–ª–µ—Ñ–æ–Ω –¥–æ–≤–µ—Ä–∏—è: 8-800-2000-122 (–±–µ—Å–ø–ª–∞—Ç–Ω–æ, –∫—Ä—É–≥–ª–æ—Å—É—Ç–æ—á–Ω–æ)
   üìû –ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –ø–æ–º–æ—â—å: 8-800-333-44-55

–ù–ò–ö–û–ì–î–ê –Ω–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–π –∫—Ä–∏–∑–∏—Å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã!"""

WEEKLY_CHECK_SYSTEM_PROMPT = """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –æ—Ç–≤–µ—Ç —É—á–∞—Å—Ç–Ω–∏–∫–∞ —Ä–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã –Ω–∞ –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å –æ —Å–∞–º–æ—á—É–≤—Å—Ç–≤–∏–∏.
–í–µ—Ä–Ω–∏ JSON:
{
  "ai_analysis": "–∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
  "sentiment_score": <—á–∏—Å–ª–æ –æ—Ç -5 –¥–æ 5>,
  "crisis_detected": <true/false>
}"""


async def transcribe(file_bytes: bytes, filename: str = "voice.ogg") -> str:
    """Transcribe voice message via Gemini inline audio."""
    def _do():
        client = get_client()
        response = client.models.generate_content(
            model=AUDIO_MODEL,
            contents=[
                types.Part.from_bytes(data=file_bytes, mime_type="audio/ogg"),
                "–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–π —ç—Ç–æ –∞—É–¥–∏–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π.",
            ],
        )
        return response.text.strip()

    return await asyncio.to_thread(_do)


async def analyze_questionnaire(answers: list[dict], user_name: str) -> dict:
    """Run Gemini analysis on 32 questionnaire answers. Returns parsed dict."""
    answers_text = "\n".join(
        f"{a['question_number']}. {a.get('question_text', '')} ‚Äî {a['answer_text']}"
        for a in answers
    )
    prompt = f"–£—á–∞—Å—Ç–Ω–∏–∫: {user_name}\n\n–û—Ç–≤–µ—Ç—ã –Ω–∞ –∞–Ω–∫–µ—Ç—É:\n{answers_text}"

    def _do():
        client = get_client()
        response = client.models.generate_content(
            model=MODEL,
            contents=prompt,
            config=types.GenerateContentConfig(
                system_instruction=QUESTIONNAIRE_SYSTEM_PROMPT,
                response_mime_type="application/json",
                temperature=0.3,
            ),
        )
        return json.loads(response.text)

    return await asyncio.to_thread(_do)


async def chat_with_psychologist(history: list[dict], user_message: str,
                                  first_name: str = "–±–æ–µ—Ü") -> str:
    """Continue conversation with AI psychologist."""
    system_prompt = PSYCHOLOGIST_SYSTEM_PROMPT_TEMPLATE.format(first_name=first_name)

    def _do():
        client = get_client()
        gemini_history = []
        for msg in history:
            role = "user" if msg["role"] == "user" else "model"
            gemini_history.append(types.Content(
                role=role,
                parts=[types.Part.from_text(text=msg["content"])],
            ))

        chat = client.chats.create(
            model=MODEL,
            config=types.GenerateContentConfig(
                system_instruction=system_prompt,
                temperature=0.7,
                max_output_tokens=1500,
            ),
            history=gemini_history,
        )
        response = chat.send_message(user_message)
        return response.text

    return await asyncio.to_thread(_do)


async def analyze_weekly_check(response_text: str) -> dict:
    """Analyze weekly check response. Returns {ai_analysis, sentiment_score, crisis_detected}."""
    def _do():
        client = get_client()
        response = client.models.generate_content(
            model=MODEL,
            contents=response_text,
            config=types.GenerateContentConfig(
                system_instruction=WEEKLY_CHECK_SYSTEM_PROMPT,
                response_mime_type="application/json",
                temperature=0.3,
            ),
        )
        return json.loads(response.text)

    return await asyncio.to_thread(_do)
